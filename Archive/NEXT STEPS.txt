Create a Crawler on Redshift stagin table

For Crawler to run on Redshift staging table, you need to create a JDBC Connection

For this, go to Data Catalog -> Connections -> create a connection using Amamzon Redshift as the Connection type and by providing the connection details

Use this connector in the Crawler and create Crawler

When running the Crawler, i got below error:

VPC S3 endpoint validation failed for SubnetId: subnet-006d417e0769d0d1f. VPC: vpc-097cd07081c9415ca. Reason: Could not find S3 endpoint or NAT gateway for subnetId: subnet-006d417e0769d0d1f in Vpc vpc-097cd07081c9415ca (Service: AWSGlueJobExecutor; Status Code: 400; Error Code: InvalidInputException; Request ID: 3351b4df-6422-4bc3-a84a-de9c54cd7359; Proxy: null)

So created an S3 endpoint of type Gateway and then ran the crawler. It ran to success and created taable in Glue Data Cataalog

This is one time manual task

------------

Use this table in Glue Spark ETL job. Enable job bookmark and transformation ctx

After S3 staging layer is loaded and crawler is run, this job must run

Then modify the procedure to load only temp & final tables and this can be in Glue Python job

Last 2 steps can be in a workflow