The spark code splits one row into multiple rows - the number of rows for each order is equal to the number of products in each order.
Also, data is partitioned by year / month / day / hour of purchase and then stored in parquet format in S3 (staging area).

Spark code is developed locally first.

Refer C:\Vinod\SparkLocalDevelopment -> 1. GOOD Local Environment Setup (from another course Spark Streaming) & How to execute locally.txt for the steps to setup spark and execute locally.
In C:\Vinod\SparkLocalDevelopmentFullPipelineProject, util, py, util.zip and app.py were first created with basic skeleton: get spark session and run a simple current_date().show() command. 
This new folder was created because of the spaces in C:\Vinod\AWS DataEngineering\8. FO to DW\Code.
The command used to test the job locally is:
spark-submit --deploy-mode client --py-files util.zip app.py orders parquet output
Here orders, parquet, output are parameters passed to the job and represent the appName, tgt_file_format and tgt_bucket

Once the basic skeleton works locally, we download a file from S3 locally and develop transformations on the local file step by step.
Once all the steps are working locally, we move the local code to Glue.
Create job using Spark script editor -> The glue job created is orders_processor_landing_to_staging.

Move all your local code to before the "job.commit()" statement.
Move all your functions in util.py to above the def core() statement.
We do not need the get_spark_session function because we will be using the spark created by the boilerplate glue code -> so we will remove the definition & invocation of get_spark_session.
Also, we will not be passing arguments on the command line like we did for local development. We will instead be using Job Parameters on the Glue console:
So below chunk of code will change:

if __name__ == '__main__':
    appName = sys.argv[1]
    tgt_file_format = sys.argv[2]
    tgt_bucket = sys.argv[3]
    core(appName, tgt_file_format, tgt_bucket)


to:

if __name__ == '__main__':
    core()

The Job Parameters added in the Glue conssole are:

--appName  orders
--tgt_file_format parquet
--tgt_bucket s3://vinod-streaming-project-staging-bucket/

Also, def core(appName, tgt_file_format, tgt_bucket): will change to: def core():

Also, remove all the show(), printSchema() commands now.