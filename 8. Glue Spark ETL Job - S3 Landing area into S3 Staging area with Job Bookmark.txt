The spark code splits one row into multiple rows - the number of rows for each order is equal to the number of products in each order.
Also, data is partitioned by year / month / day / hour of purchase and then stored in parquet format in S3 (staging area).

Spark code is developed locally first.

Refer C:\Vinod\SparkLocalDevelopment -> 1. GOOD Local Environment Setup (from another course Spark Streaming) & How to execute locally.txt for the steps to setup spark and execute locally.
In C:\Vinod\SparkLocalDevelopmentFullPipelineProject, util, py, util.zip and app.py were first created with basic skeleton: get spark session and run a simple current_date().show() command. 
This new folder was created because of the spaces in C:\Vinod\AWS DataEngineering\8. FO to DW\Code.
The command used to test the job locally is:
spark-submit --deploy-mode client --py-files util.zip app.py orders parquet output
Here orders, parquet, output are parameters passed to the job and represent the appName, tgt_file_format and tgt_bucket

Once the basic skeleton works locally, we download a file from S3 locally and develop transformations on the local file step by step.
Once all the steps are working locally, we move the local code to Glue.
Create job using Spark script editor -> The glue job created is orders_processor_landing_to_staging.

Move all your local code to before the "job.commit()" statement.
Move all your functions in util.py to above the def core() statement.
We do not need the get_spark_session function because we will be using the spark created by the boilerplate glue code -> so we will remove the definition & invocation of get_spark_session.
Also, we will not be passing arguments on the command line like we did for local development. We will instead be using Job Parameters on the Glue console:
So below chunk of code will change:

if __name__ == '__main__':
    appName = sys.argv[1]
    tgt_file_format = sys.argv[2]
    tgt_bucket = sys.argv[3]
    core(appName, tgt_file_format, tgt_bucket)


to:

if __name__ == '__main__':
    core()

The Job Parameters added in the Glue conssole are:

--appName  orders
--tgt_file_format parquet
--tgt_bucket s3://vinod-streaming-project-staging-bucket/

Also, def core(appName, tgt_file_format, tgt_bucket): will change to: def core():

Also, remove all the show(), printSchema() commands now.

Finally, because we need to use Job bookmark in the Glue job, convert the spark dataframes to dynamic frames.
Also, change the way data source is read from and data sink is written into.
For the data source and data sink syntax, you could create a test, passthrough glue job with no transformations.
Use teh data source and data sink portion alone in your job.
In the rest of the transformations, convert spark df to dynamaic frame and vice versa at each step.
Also, i removed all the functions. Just had the steps one after the other.
Only after all these changes were done, the job bookmark worked as expected.
