This is a complete ETL data pipeline that processes data from the front office (i.e. from the User Interface (UI) where order details are captured) all the way into the redshift datawarehouse. Data is transformed, compressed and enriched along the way. AWS, PySpark, Python are used here. The AWS services covered here are API Gateway, Lambda, Kinesis data firehose, kinesis delivery stream, S3, DynamoDB, Redshift, Glue (crawler, jobs with job bookmarks, workflow), Athena, EventBridge and Redshift. Spark on Glue is used for data processsing. Redshift stored procedure is also used. The "Code" folder has the code for all the services used. The main folder has the notes, important points to be taken care of. How data validation is performed for the pipeline and how potential duplicates are handled is also mentioned in the main folder.
